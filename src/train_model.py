"""
–ù–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ –¥–ª—è —Ä–æ–∑–ø—ñ–∑–Ω–∞–≤–∞–Ω–Ω—è –≥–æ–ª–æ—Å–æ–≤–∏—Ö –∫–æ–º–∞–Ω–¥
–ü–æ–≤–Ω–∏–π —Ü–∏–∫–ª: –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö ‚Üí –Ω–∞–≤—á–∞–Ω–Ω—è ‚Üí –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader
import time
import os
import sys

# –î–æ–¥–∞—î–º–æ —à–ª—è—Ö –¥–æ –Ω–∞—à–∏—Ö –º–æ–¥—É–ª—ñ–≤
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from src.simple_data_loader import create_simple_data_loaders
from src.model import SimpleCNN, EvenSimplerCNN

class Trainer:
    """
    –ö–ª–∞—Å –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ
    """
    
    def __init__(self, model, device='cpu'):
        self.model = model.to(device)
        self.device = device
        
        # –§—É–Ω–∫—Ü—ñ—è –≤—Ç—Ä–∞—Ç –¥–ª—è –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó
        self.criterion = nn.CrossEntropyLoss()
        
        # –û–ø—Ç–∏–º—ñ–∑–∞—Ç–æ—Ä Adam (–∞–¥–∞–ø—Ç–∏–≤–Ω–∏–π –≥—Ä–∞–¥—ñ—î–Ω—Ç–Ω–∏–π —Å–ø—É—Å–∫)
        self.optimizer = optim.Adam(
            self.model.parameters(), 
            lr=0.001,           # –®–≤–∏–¥–∫—ñ—Å—Ç—å –Ω–∞–≤—á–∞–Ω–Ω—è
            weight_decay=1e-4   # –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—è (–∑–∞–ø–æ–±—ñ–≥–∞—î –ø–µ—Ä–µ–Ω–∞–≤—á–∞–Ω–Ω—é)
        )
        
        # –î–ª—è –∑–±–µ—Ä—ñ–≥–∞–Ω–Ω—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫
        self.train_losses = []
        self.train_accuracies = []
        
        print(f"üéØ –¢—Ä–µ–Ω–µ—Ä —Å—Ç–≤–æ—Ä–µ–Ω–æ!")
        print(f"   üì± –ü—Ä–∏—Å—Ç—Ä—ñ–π: {device}")
        print(f"   üí• –§—É–Ω–∫—Ü—ñ—è –≤—Ç—Ä–∞—Ç: CrossEntropyLoss")
        print(f"   üèÉ –û–ø—Ç–∏–º—ñ–∑–∞—Ç–æ—Ä: Adam (lr=0.001)")
    
    def train_epoch(self, train_loader):
        """–ù–∞–≤—á–∞–Ω–Ω—è –Ω–∞ –æ–¥–Ω—ñ–π –µ–ø–æ—Å—ñ (–ø–æ–≤–Ω–∏–π –ø—Ä–æ—Ö—ñ–¥ –ø–æ –≤—Å—ñ—Ö –¥–∞–Ω–∏—Ö)"""
        
        self.model.train()  # –†–µ–∂–∏–º –Ω–∞–≤—á–∞–Ω–Ω—è
        
        epoch_loss = 0.0
        correct = 0
        total = 0
        
        for batch_idx, (data, target) in enumerate(train_loader):
            # –ü–µ—Ä–µ–Ω–æ—Å–∏–º–æ –¥–∞–Ω—ñ –Ω–∞ –ø—Ä–∏—Å—Ç—Ä—ñ–π (CPU/GPU)
            data, target = data.to(self.device), target.to(self.device)
            
            # –û–±–Ω—É–ª—è—î–º–æ –≥—Ä–∞–¥—ñ—î–Ω—Ç–∏
            self.optimizer.zero_grad()
            
            # –ü—Ä—è–º–∏–π –ø—Ä–æ—Ö—ñ–¥: –¥–∞–Ω—ñ ‚Üí –º–æ–¥–µ–ª—å ‚Üí –ø—Ä–æ–≥–Ω–æ–∑–∏
            output = self.model(data)
            
            # –û–±—á–∏—Å–ª—é—î–º–æ –≤—Ç—Ä–∞—Ç–∏ (–Ω–∞—Å–∫—ñ–ª—å–∫–∏ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ñ –ø—Ä–æ–≥–Ω–æ–∑–∏)
            loss = self.criterion(output, target)
            
            # –ó–≤–æ—Ä–æ—Ç–Ω–∏–π –ø—Ä–æ—Ö—ñ–¥: –≤—Ç—Ä–∞—Ç–∏ ‚Üí –≥—Ä–∞–¥—ñ—î–Ω—Ç–∏
            loss.backward()
            
            # –û–Ω–æ–≤–ª—é—î–º–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –º–æ–¥–µ–ª—ñ
            self.optimizer.step()
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            epoch_loss += loss.item()
            _, predicted = torch.max(output.data, 1)
            total += target.size(0)
            correct += (predicted == target).sum().item()
            
            # –ü–æ–∫–∞–∑—É—î–º–æ –ø—Ä–æ–≥—Ä–µ—Å –∫–æ–∂–Ω—ñ 100 –±–∞—Ç—á—ñ–≤
            if batch_idx % 100 == 0:
                print(f'   –ë–∞—Ç—á {batch_idx}/{len(train_loader)}: '
                      f'–í—Ç—Ä–∞—Ç–∏={loss.item():.4f}, '
                      f'–¢–æ—á–Ω—ñ—Å—Ç—å={100.*correct/total:.1f}%')
        
        # –°–µ—Ä–µ–¥–Ω—ñ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∑–∞ –µ–ø–æ—Ö—É
        avg_loss = epoch_loss / len(train_loader)
        accuracy = 100. * correct / total
        
        self.train_losses.append(avg_loss)
        self.train_accuracies.append(accuracy)
        
        return avg_loss, accuracy
    
    def evaluate(self, test_loader):
        """–û—Ü—ñ–Ω–∫–∞ –º–æ–¥–µ–ª—ñ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö"""
        
        self.model.eval()  # –†–µ–∂–∏–º –æ—Ü—ñ–Ω–∫–∏ (–≤–∏–º–∏–∫–∞—î–º–æ dropout —Ç–æ—â–æ)
        
        test_loss = 0
        correct = 0
        total = 0
        
        # –ö–ª–∞—Å–∏ –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ—ó —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        classes = ['yes', 'no', 'up', 'down']
        class_correct = [0] * 4
        class_total = [0] * 4
        
        with torch.no_grad():  # –ù–µ –æ–±—á–∏—Å–ª—é—î–º–æ –≥—Ä–∞–¥—ñ—î–Ω—Ç–∏ (–µ–∫–æ–Ω–æ–º–∏–º–æ –ø–∞–º'—è—Ç—å)
            for data, target in test_loader:
                data, target = data.to(self.device), target.to(self.device)
                
                output = self.model(data)
                test_loss += self.criterion(output, target).item()
                
                _, predicted = torch.max(output, 1)
                total += target.size(0)
                correct += (predicted == target).sum().item()
                
                # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –∫–ª–∞—Å–∞—Ö
                for i in range(len(target)):
                    label = target[i].item()
                    class_total[label] += 1
                    if predicted[i] == target[i]:
                        class_correct[label] += 1
        
        avg_loss = test_loss / len(test_loader)
        accuracy = 100. * correct / total
        
        # –ü–æ–∫–∞–∑—É—î–º–æ –¥–µ—Ç–∞–ª—å–Ω—É —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        print(f"\nüìä –î–µ—Ç–∞–ª—å–Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–ª–∞—Å–∞—Ö:")
        for i, class_name in enumerate(classes):
            if class_total[i] > 0:
                class_acc = 100. * class_correct[i] / class_total[i]
                print(f"   {class_name}: {class_acc:.1f}% ({class_correct[i]}/{class_total[i]})")
        
        return avg_loss, accuracy

def train_model(model_type='simple', epochs=3, batch_size=32, save_model=True):
    """
    –û—Å–Ω–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è –Ω–∞–≤—á–∞–Ω–Ω—è
    
    Args:
        model_type: 'simple' –∞–±–æ 'even_simpler'
        epochs: –∫—ñ–ª—å–∫—ñ—Å—Ç—å –µ–ø–æ—Ö –Ω–∞–≤—á–∞–Ω–Ω—è
        batch_size: —Ä–æ–∑–º—ñ—Ä –±–∞—Ç—á–∞
        save_model: —á–∏ –∑–±–µ—Ä—ñ–≥–∞—Ç–∏ –º–æ–¥–µ–ª—å –ø—ñ—Å–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è
    """
    
    print("üöÄ –ü–æ—á–∏–Ω–∞—î–º–æ –Ω–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ!")
    print("=" * 50)
    
    # –í–∏–∑–Ω–∞—á–∞—î–º–æ –ø—Ä–∏—Å—Ç—Ä—ñ–π (CPU –±–æ —É –Ω–∞—Å –Ω–µ–º–∞—î GPU)
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"üñ•Ô∏è –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –ø—Ä–∏—Å—Ç—Ä—ñ–π: {device}")
    
    # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –¥–∞–Ω—ñ
    print("\nüìÇ –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –¥–∞–Ω—ñ...")
    train_loader, test_loader = create_simple_data_loaders(batch_size=batch_size)
    
    if train_loader is None:
        print("‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö!")
        return None
    
    # –°—Ç–≤–æ—Ä—é—î–º–æ –º–æ–¥–µ–ª—å
    print(f"\nüèóÔ∏è –°—Ç–≤–æ—Ä—é—î–º–æ –º–æ–¥–µ–ª—å: {model_type}")
    if model_type == 'simple':
        model = SimpleCNN(num_classes=4)
    else:
        model = EvenSimplerCNN(num_classes=4)
    
    # –°—Ç–≤–æ—Ä—é—î–º–æ —Ç—Ä–µ–Ω–µ—Ä
    trainer = Trainer(model, device)
    
    # –û—Ü—ñ–Ω–∫–∞ –¥–æ –Ω–∞–≤—á–∞–Ω–Ω—è
    print(f"\nüß™ –¢–µ—Å—Ç—É—î–º–æ –º–æ–¥–µ–ª—å –î–û –Ω–∞–≤—á–∞–Ω–Ω—è...")
    initial_loss, initial_acc = trainer.evaluate(test_loader)
    print(f"–ü–æ—á–∞—Ç–∫–æ–≤–∞ —Ç–æ—á–Ω—ñ—Å—Ç—å: {initial_acc:.1f}% (–≤–∏–ø–∞–¥–∫–æ–≤–∞ = 25%)")
    
    # –ù–∞–≤—á–∞–Ω–Ω—è
    print(f"\nüéì –ü–æ—á–∏–Ω–∞—î–º–æ –Ω–∞–≤—á–∞–Ω–Ω—è –Ω–∞ {epochs} –µ–ø–æ—Ö...")
    start_time = time.time()
    
    best_accuracy = 0.0
    
    for epoch in range(epochs):
        print(f"\n--- –ï–ø–æ—Ö–∞ {epoch + 1}/{epochs} ---")
        
        # –ù–∞–≤—á–∞—î–º–æ –æ–¥–Ω—É –µ–ø–æ—Ö—É
        train_loss, train_acc = trainer.train_epoch(train_loader)
        
        # –û—Ü—ñ–Ω—é—î–º–æ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö
        test_loss, test_acc = trainer.evaluate(test_loader)
        
        print(f"üìà –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ –µ–ø–æ—Ö–∏ {epoch + 1}:")
        print(f"   –¢—Ä–µ–Ω—É–≤–∞–ª—å–Ω–∞ –≤—Ç—Ä–∞—Ç–∞: {train_loss:.4f}")
        print(f"   –¢—Ä–µ–Ω—É–≤–∞–ª—å–Ω–∞ —Ç–æ—á–Ω—ñ—Å—Ç—å: {train_acc:.1f}%")
        print(f"   –¢–µ—Å—Ç–æ–≤–∞ –≤—Ç—Ä–∞—Ç–∞: {test_loss:.4f}")
        print(f"   –¢–µ—Å—Ç–æ–≤–∞ —Ç–æ—á–Ω—ñ—Å—Ç—å: {test_acc:.1f}%")
        
        # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –Ω–∞–π–∫—Ä–∞—â—É –º–æ–¥–µ–ª—å
        if test_acc > best_accuracy:
            best_accuracy = test_acc
            if save_model:
                os.makedirs('./models', exist_ok=True)
                torch.save(model.state_dict(), f'./models/best_model_{model_type}.pth')
                print(f"üíæ –ó–±–µ—Ä–µ–∂–µ–Ω–æ –∫—Ä–∞—â—É –º–æ–¥–µ–ª—å (—Ç–æ—á–Ω—ñ—Å—Ç—å: {test_acc:.1f}%)")
    
    training_time = time.time() - start_time
    
    # –ü—ñ–¥—Å—É–º–∫–∏
    print(f"\nüéâ –ù–∞–≤—á–∞–Ω–Ω—è –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    print("=" * 50)
    print(f"‚è±Ô∏è –ß–∞—Å –Ω–∞–≤—á–∞–Ω–Ω—è: {training_time:.1f} —Å–µ–∫—É–Ω–¥")
    print(f"üéØ –ù–∞–π–∫—Ä–∞—â–∞ —Ç–æ—á–Ω—ñ—Å—Ç—å: {best_accuracy:.1f}%")
    print(f"üìà –ü–æ–∫—Ä–∞—â–µ–Ω–Ω—è: {best_accuracy - initial_acc:.1f}%")
    
    # –í–∏–º—ñ—Ä—é—î–º–æ —à–≤–∏–¥–∫—ñ—Å—Ç—å —ñ–Ω—Ñ–µ—Ä–µ–Ω—Å—É
    print(f"\n‚ö° –¢–µ—Å—Ç—É—î–º–æ —à–≤–∏–¥–∫—ñ—Å—Ç—å —ñ–Ω—Ñ–µ—Ä–µ–Ω—Å—É...")
    model.eval()
    with torch.no_grad():
        # –ë–µ—Ä–µ–º–æ –æ–¥–∏–Ω –±–∞—Ç—á –¥–ª—è —Ç–µ—Å—Ç—É
        test_data, _ = next(iter(test_loader))
        test_data = test_data.to(device)
        
        # –í–∏–º—ñ—Ä—é—î–º–æ —á–∞—Å –Ω–∞ 100 –ø—Ä–æ–≥–Ω–æ–∑—ñ–≤
        start_time = time.time()
        for _ in range(100):
            _ = model(test_data[:1])  # –û–¥–∏–Ω –∑—Ä–∞–∑–æ–∫
        inference_time = (time.time() - start_time) * 1000 / 100  # –º—Å –Ω–∞ –∑—Ä–∞–∑–æ–∫
    
    print(f"üî• Latency: {inference_time:.1f} –º—Å –Ω–∞ –∑—Ä–∞–∑–æ–∫")
    
    return model, best_accuracy, inference_time

if __name__ == "__main__":
    import argparse
    
    # –ü–∞—Ä—Å–∏–Ω–≥ –∞—Ä–≥—É–º–µ–Ω—Ç—ñ–≤ –∫–æ–º–∞–Ω–¥–Ω–æ–≥–æ —Ä—è–¥–∫–∞
    parser = argparse.ArgumentParser(description='Train Speech Commands Model')
    parser.add_argument('--epochs', type=int, default=3, help='Number of training epochs')
    parser.add_argument('--batch-size', type=int, default=32, help='Batch size for training')
    parser.add_argument('--model-type', type=str, default='simple', choices=['simple', 'even_simpler'], help='Model architecture')
    args = parser.parse_args()
    
    print("üéì –°–∫—Ä–∏–ø—Ç –Ω–∞–≤—á–∞–Ω–Ω—è Speech Commands –º–æ–¥–µ–ª—ñ")
    print("=" * 50)
    
    # –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –Ω–∞–≤—á–∞–Ω–Ω—è –∑ –∞—Ä–≥—É–º–µ–Ω—Ç—ñ–≤
    EPOCHS = args.epochs
    BATCH_SIZE = args.batch_size
    MODEL_TYPE = args.model_type
    
    print(f"‚öôÔ∏è –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è:")
    print(f"   –ï–ø–æ—Ö–∏: {EPOCHS}")
    print(f"   –†–æ–∑–º—ñ—Ä –±–∞—Ç—á–∞: {BATCH_SIZE}")
    print(f"   –¢–∏–ø –º–æ–¥–µ–ª—ñ: {MODEL_TYPE}")
    
    # –ó–∞–ø—É—Å–∫–∞—î–º–æ –Ω–∞–≤—á–∞–Ω–Ω—è
    try:
        model, accuracy, latency = train_model(
            model_type=MODEL_TYPE,
            epochs=EPOCHS,
            batch_size=BATCH_SIZE,
            save_model=True
        )
        
        if model is not None:
            print(f"\n‚úÖ –£—Å–ø—ñ—à–Ω–æ –Ω–∞–≤—á–µ–Ω–æ –º–æ–¥–µ–ª—å!")
            print(f"üìä –§—ñ–Ω–∞–ª—å–Ω—ñ –º–µ—Ç—Ä–∏–∫–∏:")
            print(f"   üéØ Accuracy: {accuracy:.1f}%")
            print(f"   ‚ö° Latency: {latency:.1f} –º—Å")
            
            # –†–æ–∑–º—ñ—Ä –º–æ–¥–µ–ª—ñ
            model_size = sum(p.numel() for p in model.parameters()) * 4 / 1024  # KB
            print(f"   üíæ –†–æ–∑–º—ñ—Ä –º–æ–¥–µ–ª—ñ: {model_size:.1f} KB")
            
            print(f"\nüéä –ú–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞ –¥–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è!")
        
    except Exception as e:
        print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—ñ–¥ —á–∞—Å –Ω–∞–≤—á–∞–Ω–Ω—è: {e}")
        import traceback
        traceback.print_exc()